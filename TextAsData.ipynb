{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming Text into Features for Sentiment AnalysisÂ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, you will see how to create a classifier that performs sentiment analysis of a book review. \n",
    "You will learn how to use scikit-learn to convert raw text data (such as a book review) into a matrix of <i>term frequency-inverse document frequency</i> (TF-IDF) features, and how to train a logistic regression model using these transformed features. You will also experiment with using different document-frequency values and see how they affect the performance of a logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages\n",
    "\n",
    "Before you get started, import a few packages. Run the code cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also import the scikit-learn `LogisticRegression`, the `train_test_split()` function for splitting the data into training and test sets, and the function `roc_auc_score` to evaluate the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load a 'ready-to-fit' Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with a data set containing book reviews taken from Amazon.com reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(os.getcwd(), \"data\", \"bookReviews.csv\")\n",
    "df = pd.read_csv(filename, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Positive Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This was perhaps the best of Johannes Steinhof...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This very fascinating book is a story written ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The four tales in this collection are beautifu...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The book contained more profanity than I expec...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We have now entered a second time of deep conc...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Positive Review\n",
       "0  This was perhaps the best of Johannes Steinhof...             True\n",
       "1  This very fascinating book is a story written ...             True\n",
       "2  The four tales in this collection are beautifu...             True\n",
       "3  The book contained more profanity than I expec...            False\n",
       "4  We have now entered a second time of deep conc...             True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Training and Test Data Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Labeled Examples\n",
    "\n",
    "Let's create labeled examples from our dataset. We will have one text feature and one label. \n",
    "The code cell below carries out the following steps:\n",
    "\n",
    "* Gets the `Positive_Review` column from DataFrame `df` and assign it to the variable `y`. This will be our label. Note that the label contains True or False values that indicate whether a given book review is a positive one.\n",
    "* Gets the column `Review` from DataFrame `df` and assigns it to the variable `X`. This will be our feature. Note that the `Review` feature contains the book review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1973,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['Positive Review'] \n",
    "X = df['Review']\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    This was perhaps the best of Johannes Steinhof...\n",
       "1    This very fascinating book is a story written ...\n",
       "2    The four tales in this collection are beautifu...\n",
       "3    The book contained more profanity than I expec...\n",
       "4    We have now entered a second time of deep conc...\n",
       "Name: Review, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at an example of a positive and a negative review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Positive Review: \n",
      "\n",
      " I am not going to go over the contents of the book, or much about Charles Bukowski, because if you are considering this book you must know something about the man and his work. I will just give you my impression of this collection of work.\n",
      "No collection can ever really be complete, there are always new things to add, new commentary, newly discovered works, transcripts of records and unpublished letters, but this book does an excellent job in its attempt.\n",
      "To me Charles Bukowski will always be one of the greatest American writers of the twentieth century, because of the sheer brutality and honesty his work emanates. It is funny, sad, sadistic, cruel, scathing, enlightening and thought provoking. Everything I like to read. This is poetry for people who are disgusted by verse of flowers, trees and Greek mythology. This is RAW human emotion and experience smeared out onto paper. It is not perfect, and it is not trying to be. It doesn't always work, but there in lies the subtle beauty of Bukowski's efforts. the guts to try. The attempts at honesty, clearly blocked by his unwillingness to divulge everything, and his cynicism of man. \n",
      "This book has one of the most moving, amazing, and insightful poems...or anything else...ever written.  It is called The Genius Of The Crowd. If you read that work of art and are not moved...nothing will.\n",
      "This collection is shocking in its beauty, and inspiring by its simplicity. Enjoy\n",
      "\n",
      "A Negative Review: \n",
      "\n",
      " Having read a few Bosch books already (The Closers and Lost Light), I realized that I should probably start at the beginning to get better insight on how Bosch became the Bosch I was reading in the later series.  However, I was very disappointed with this book.  One of the most irritating aspects of this book was the grammatical errors (mispellings and such).  I found 3 or 4 in the first 50 pages (before I stopped reading)!  I thought it was ironic that Connelly was thanking his editor at the beginning of the book, because it did not seem that anyone was editing this book.  Also, I found the book to be too gritty.  It lacked the emotion and soul that I found in the later books.  It seemed that Connelly was trying too hard to make Bosch fit this certain mold, but then transformed the character to be more likeable in the later books that I read.  So it was disappointing that this book could not hold my interest because I just finished reading The Narrows and loved it, and will probably pick up some other Bosch books and just read it out of order.  Connelly's writing seems to be getting better and better, which should explain why I didn't enjoy this book and couldn't finish it.  I gave it 2 stars because it might have gotten better and maybe I was too impatient.  This is one of those series that seems like it is better to read out of order\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('A Positive Review: \\n\\n', X[67])\n",
    "print('A Negative Review: \\n\\n', X[85])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Labeled Examples into Training and Test Sets\n",
    "\n",
    "Let's split our data into training and test sets with 75% of the data being the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500     There is a reason this book has sold over 180,...\n",
       "1047    There is one thing that every cookbook author ...\n",
       "1667    Being an engineer in the aerospace industry I ...\n",
       "1646    I have no idea how this book has received the ...\n",
       "284     It is almost like dream comes true when I saw ...\n",
       "Name: Review, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.75, random_state=1234)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3:  Implement TF-IDF Vectorizer to Transform Text\n",
    "\n",
    "A popular technique when transforming text to numerical feature vectors is to use the TF-IDF statistical measure. TF-IDF calculates how relevant a word is in a document relative to a collection of documents. It weighs words to indicate the words that are the most unique to the document and therefore can be used to represent the characteristics of the document. For example, the word \"the\" appears in many documents and therefore is not characteristic of one particular document in a collection. On the other hand, if a word appears often in one document and rarely in other documents in the collection, the word is given a higher value of importance to that one document. \n",
    "Because TF-IDF provides an understanding of the context of the textual data, using TF-IDF features when performing classification for sentiment analysis yields more accurate results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a simple example. We will use the scikit-learn `TfidfVectorizer` class to implement a TF-IDF vectorizer. For more information, consult the online [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). First, let's import `TfidfVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the values in the TF-IDF matrix that `TfidfVectorizer` produces:\n",
    "\n",
    "* <b>Row</b>: each document will be represented by a numerical vector (row) in the matrix. \n",
    "* <b>Column</b>: each column represents one word in the vocabulary, i.e. the number of words in ALL of the documents in the collection (with the exclusion of words that appear too frequently or too infrequently; scikit-learn has a list of such words to ignore by default, but you will see later that you can specify frequency thresholds to eliminate words that appear too often/little). \n",
    "    * The value in the columns are the TF-IDF scores (weights) for the word in every document in the collection (one document per row)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell below transforms two \"documents.\" Run the cell below to see what the code produces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size 7: {'my': 5, 'cat': 1, 'loves': 4, 'yarn': 6, 'blue': 0, 'have': 3, 'dog': 2}\n",
      "\n",
      "Matrix:\n",
      "\n",
      "[[0.25969799 0.36499647 0.         0.         0.36499647 0.36499647\n",
      "  0.72999294]\n",
      " [0.44943642 0.         0.6316672  0.6316672  0.         0.\n",
      "  0.        ]]\n",
      "\n",
      "Heatmap of Matrix:\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqAAAAE/CAYAAACZ5HHOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAA0JElEQVR4nO3debxVZb3H8c/vAAoOgIyCE445lilqlvP1mqWUXhWsrNTUZoc0r5n3pqbXKYfKyjKNHFKcE81ZUXGexVkSB2YBUVFB4Dz3j2cd2GzOBAfW5ujn/XqdF3ut9ay1f3vttc/57mc9axEpJSRJkqSy1NW6AEmSJH26GEAlSZJUKgOoJEmSSmUAlSRJUqkMoJIkSSqVAVSSJEmlMoBKkqRFFhHHR8Rfa12H2icDqCRJzYiIGRU/9RHxUcX0tyLixIiYXdXu2Ca2dWBEjKyYfr3Y3vsRMT0iHoyIH0REXUWboRHxcdX2hzSx/RQRkyOiY8W8TsW8Vt34OyJ2ioixLbVLKf1fSumQ1mxTqmYAlSSpGSmllRp+gDeBQRXzLi+aDatsl1I6cxGeYlBKaWVgLeB04L+Bi6ranFm1/WHNbO8d4CsV018p5i0xlQFXWhwGUEmSlgEppXdTSjcCQ4DvRsSmi7mpS4HvVEx/B7ikskFEHBQRLxY9r69FxPeL+SsCtwD9K3pb+xe9vNdExGUR8R5wYDHvsmK9IRExJiK6FtNfiYiJEdF7MV+DPuEMoJIkLUNSSo8CY4HtF3MTNwA7RET3iFil2M4/q9pMBvYEugIHAedGxBYppQ/IPabjK3pbxxfrfB24BugOXF65saJH9kHgdxHRk9yDe0hK6e3FfA36hDOASpLUdoOLMZwNP/3buL3xQI+K6WMqtj2lhXVnAsPJPalDgBuLefOklG5OKf07ZfcCt9Ny4H0opXRDSqk+pfRRI8t/DOwCjACGp5RuamF7+hQzgEqS1HZXpZS6V/yMj4jtK05jP7+I21sNmFYx/ZuKbfdqxfqXkE+9L3T6HeadIn84IqZFxHTgq0BL232ruYUppenA1cCmwNmtqFGfYgZQSZKWgpTS/RWnsTdp7XoRsRU5gI5sqW0z7gf6AX2rtxMRywPXAr8B+qaUugP/AqKh9Ca22exV9BGxOXAwcAXwu8WsW58SBlBJkpYBEdE1IvYErgQuSymNWtxtpZQSMAj4WvG40nLA8sDbwJyI+AqwW8XySUDPiOi2CLV3Bi4DjiePKV0tIn60uPXrk8/bKEiSVFvDI2IOUA+8AJwDXNDWjaaUGj3tn1J6PyIOB64iB9Hh5HGiDctfiogrgNciogOwcSue7jTgrZTSnwAi4gDgnoi4I6X0ahtfij6BYuEvRpIkSdLS4yl4SZIklcoAKkmSpFIZQCVJklQqA6gkSZJKZQCVJElSqbwNkyS1YPgTc9r17UI26f5GrUtYbN85cmytS/hUu+S81Wtdwqfaixt+tdYltMkes1+OppbZAypJkqRSGUAlSZJUKgOoJEmSSmUAlSRJUqkMoJIkSSqVAVSSJEmlMoBKkiSpVAZQSZIklcoAKkmSpFIZQCVJklQqA6gkSZJKZQCVJElSqQygkiRJKpUBVJIkSaUygEqSJKlUBlBJkiSVygAqSZKkUhlAJUmSVCoDqCRJkkplAJUkSVKpDKCSJEkqlQFUkiRJpTKASpIkqVQGUEmSJJXKACpJkqRSGUAlSZJUKgOoJEmSSmUAlSRJUqkMoJIkSSqVAVSSJEmlMoBKkiSpVAZQSZIklcoAKkmSpFIZQCVJklQqA6gkSZJKZQCVJElSqQygkiRJKpUBVJIkSaUygEqSJKlUBlBJkiSVygAqSZKkUhlAJUmSVCoDqCRJkkplAJUkSVKpDKCSJEkqlQFUkiRJpTKASpIkqVQGUEmSJJXKACpJkqRSGUAlSZJUKgOoJEmSSmUAlSRJUqkMoJIkSSqVAVSSJEmlMoBKkiSpVAZQSZIklcoAKkmSpFIZQCVJklQqA6gkSZJKZQCVJElSqVoMoBFxYkSk4qc+It6JiMci4tSIWLWMIpdlETE4Ig5sZdshEXFdREwo9mer1mtiW0OLbdzZyLIuEfFeW59jSVrE/ZQi4idLuaRPpYg4sOLznCJiVkS8HBHHR0SHinYDiuV71rJeSdInU2t7QN8FtgW+COwPXAd8GxgVEVsupdrai8HAga1suy8wALhpCT33DGCniOhbNX9PIJbQcywpi7KftPTtQv5Mfxm4DPg18N81rUiS9KnRsZXt5qSUHq6Yvi0i/gTcB1wZERumlOYu+fI+cYaklOojYiXgkCWwvZeBlYH9gPMr5u8P3Ah8cwk8h2qk6JHskFL6eCls/rGU0ozi8YiI2AzYC/i/pfBcNRURXVJKH9Xq+V965n7+ecnp1NfPZZud92GXrx26wPJ7bx7KIyOupUNdR1bsugqDDzuFHr37A/DOlPFcfeGvmD51IgQccuwF9Oi9Wqn1P/7441zw5z9TX1/P7l/+MoMHD15g+c0338xNN91EXYcOdO7cmcMPP5y11lwTgDFjxvC73/+eDz/8kLoIfvvb37LccsuVWn9zttliFY44dD3q6oKb7pjAZde8VeuSFsmyXn97P3bae/29d9uejc/5JdGhjrcuvpp/n3XhAss3+s0v6LnTNgB06NKZ5fv05PbeW9Flzf5sec35UFdHXceOvP7Hy3jzL1cu8foWewxoSmk6cCywHvCfDfMjoldE/D0ipkbEhxExIiIGVq8fEYdGxKiImBkRkyLimojoViwbERHXVLXfqTgluGkx3XCKcP+I+FtxynlsRBxQLD82IsZHxNsRcUZE1FVtb9OIuDki3i9+rq4cUlDxfDsVy2ZExGsR8aOKNkOBfYAdK05pntjMPqtv/R5utWHkwNlQ08rAV4EFjpaI+FHxGlaqmt/wOj/X1BMUp/TPjIg3ilO2YyLitIrl34mIkRExrRiicU/le76o+6mJGn4SEa8Wzz86Io5q5DVsUrXOKhHxcUQcUjFv+4i4tzg2p0bEhcU+a1jePSL+Whw7MyPizYhY8FO74HO0ar+2tI8a9lNEPB4Re0XE88BMYJuK+f8ZEc9GxAfFthZ4vW30PtCpuQbRyNCIyEN0plTNWzMirixe64cRcVtEfKaZ7fYo9vWBVfOj+MydW0xvWGz3rWK7z0fEkZWf7Yr9/uWIuDEiZgDnt+bzvDTU18/l+r+dyiHHXsDPz7qRpx78FxPHjl6gzWoDNuLIU67i6DOu57Nb78bNV5w9b9kVfzqenfY8iGN/M5wjfn0lK3XtsTTLXcjcuXP5wx//yK9PPpk/X3ABI+69lzfefHOBNjvtvDN/+tOf+MP557Pfvvty4YUXzlv3zLPO4qc/+Ql/vuACzjjjDDp06NDY09REXR387Afrc8yJozjgx4+x6w59GLDGCrUuq9WW9frb+7HT3uunro5Nfve/PDroEO797B70339PVtpo3QWavHjMaYwcuBcjB+7FG3+8jIk33AHAzAlv8+B2Qxg5cC8e+NJg1v35oSzfr8+SL7GN648A5gBfqJh3A/m03jHAkOI57omI9RoaRMQJwJ+Be8m9Lj8kn+Zf4I94K50BTCAHnPuBv0fE2cDWwMHAeeSgPO+rS1HLA0Bn4ADyqeFNgOERUX3q+kLgGWDv4vX+ISK2Lpb9GrgHeIp8OnNb4K+L8RrmifnB+sBWrnIF8MWIWLOY3ht4h7xvK/0D6EAeBlDpIODJlNIzTdQTwD/J79EfyOH2V0CvimYDgEvIPbHfBN4C7o+IdYrlbdpPEXEo8Htyr+4g4Grg7Ig4rmhyH/kYGFy16t7Fv9cW2/kScCcwkbwfjixez98q1jkH2A44inwcHw+kZspr7X4dQPP7iIp2ZwKnAV8BxhTz1wTOAk4FvgH0AYY1cry2VoeI6BgRK0bEV8j77vrF3NY8EdEDGAl8BvhBsd0VgTsjoktj66SUphXPfWDVop2AtYGLi+nVyL3+PyK/bxcCJ9H40IGLyJ/brxWPGzT3eV7i3hw9ip5916Bn3zXo2HE5Nt/2qzz/xD0LtFlvk21Ybvm8a9Za/3O8O20iABPHjqZ+7hw22OyLACzfecV57cryyiuv0L9/f/r160enTp3YcYcdePihhxZos+IK80PPzJkz5439eeLJJ1l77bVZZ518iHft2nWZCqAbrd+VsRM+YvykmcyZk7jzvslst03PWpfVast6/e392Gnv9Xff+rN8+O83+GjMWNLs2YwfdjN9B/1Hk+37D9mD8Vfm0YFp9mzqP54NQN3yyxF1S+d69daegm9USmlm0fvRFyAidge+BOyUUrq3mHc38Drwc+D7EdGd/Ef9vJTSzyo2d91ilnF3Sun44rkeIQeBrwENwwJujYivk//gNPQK/oocQr7ScHozIp4FXiL/Ybu5YvtXpJROKdqMIAeg/wIeTSn9OyKmAXVVQxTaIgFzgVb1lqaUXoyIUeSwfxa5N/Sq6vVTStMj4lpyMBoKUPTa7QMcR9N2I/dwfz2ldGPF/Esqtn1yw+OiN+oO8heAA4CT27Kfiu2dCAxNKR1dzL49cm/5LyLivOI4vLrYB7+qWH0IcHtK6Z1i+nTgwZTSkIrtjwPuiohNU0rPFXX/IaU0rGI7lzVVX2v3a0v7qGKTPYFdU0pPV7QH6AF8KaX0asU2ricHvZeaqq8Z06umr2PJnH4/ihw4Ny+CJRHxAPl3wMHkLzGNuYj8vq6TUnqtmHcQ8ERKaRRASuku4K5im0EOuisAh5IDe6WrU0r/0zARETsVD5v8PC/Wq23Bu+9MonvPfvOmu/foyxujn22y/SP3XMuGn9segCkT3qDLil0Zeu4RTJs8lvU33ZY9vnEUdXXl/SGbMnUqvXvN/67Zq1cvXn755YXaDR8+nOuuv545c+Zw+mn5rRg3bhwB/PKEE3j33XfZcYcd2G+//coqvUW9ey7H5Cmz5k2/PXUWG2/QtYYVLZplvf72fuy09/o79+/LR2MnzpueOW4S3bf+bKNtu6zZny4DVmfKPfP/PHdefVW2uvEvrLjumrx43JnMmjB5ide4JGJtZQ/M1sDkhvAJkFL6gHzRzXbFrG2BLizY69QWd1U813vA28C9VWNSR5N7TxrsSv7jXV/0AnUk9zS9DlQPF7i9YvuzgVeB1ZdQ7QtJKb2RUuqYUrqk5dbzXAnsHxE9ya+tqcEaFwHbV/S6DSZ/CflHM9veBZhWFT4XEBEbRcT1ETGJHJ5nk4PRBovwGpqyOtCf3OtZaRjQFdisYvozFae8exW1DyumVyAfe1c1vOfF+z6yqLfhYrqngZ9HPrXe2vpb3K+LsI/GVYbPCq83hM/CC8W/i3ss7gBsRd4n3wO2IfcOttWu5HD9XsU+fh94goU/W5XuAt4AvgvzhpLsQ8XviYjoHBEnRcRoYBZ5H54KrF08T6WbaVypn+dF8cTI4Ywd8zw77XkwAHPr5zDmpScY9M1jOOKUYUyb/BaP3XtDbYtswqBBg/jbxRdz8EEHccWV+dfP3Llzef6FFzj25z/nN2edxYMPPcRTTz9d20K1zGnvx057rx+g3+A9mHjdbVA/v99q5tiJ3L/F17hnw91Y/dt7s1yfJd+73qYAGhGdyT02k4pZ/YDGYvIkcg8ORXvIp0yXhOlV0x83Ma9zxXQv8mm72VU/6wBrtGL7nVm2XAlsAfyCHGCa6mUcAbzG/FOdBwH/bOipakJPmnmviqBwO3m//QzYnhxsnmHJ7KeG7qNJVfMbphuOq4eAN8m9npDDyxzykBCAVcinyv/Igu/5LPLYx4b3/SfFOv8LvBx53Om8MbZNGEEz+3UR91H162wwvWq64cKkxd3HT6WUHk8pPZxSuhg4HDgwijHWbdCL/B5Uf7Z2ZuHP1jwppUQOm98tejcHk9+vyi9HZ5CH9vyFfKZiK+CUYllb9mOj+zAiDos89vbxW69bvGzebZW+TJ86/+MzfdokuvWovmkFvDLqIe664S8cdPT5dOyUL1To3mNV+q+1IT37rkGHDh3ZdOB/MO71FxZad2nq1bMnb0+ZP8R3ypQp9OzZ9B+iHXfckYeK05S9evVi0003pVu3bnTu3JmtBg7k36NHN7lu2d6e+jF9ei0/b7p3z+V5e+qsZtZYtizr9bf3Y6e91z9z/CS6rD7/TpmdV+vLzHGN/1rsP+SrjB/W+Hf2WRMm8/7zr9Jju+b6DxZPW3tAdyb39DQMjJhAHptWrS/QEHKmFv/2a6Rdg5lA9eViqyxmjY2ZRh6DulUjP6c0s94yKaU0hnwK8SiKHr8m2iXyeLrvFL1729FyT/RUmn+vtiX3IB2QUro8pTQypfQ40G0RXkJzGv56Vx9XDX/Fp8G813YV88eBDgFuSSm9X0xPJw9v+BWNv+8XF9uZnlI6PKW0KvA54BHg8ojYuKkCW7FfF2UfNTfedGl6sfh3o2bazKLlz+U08ljdxvbxj1uo4W/kkLozOczfUDF8AvL42d+nlM5MKd1Z7MM5TWyrzfsxpfSXlNLAlNLA3f/r0JZXaMQa627KlIlvMnXyWObM+ZinH/oXm2y58wJtxr3+ItdedBIHHX0+K3frucC6H334HjPey786X33+EfqutuBFBEvbBhtswPjx45k4cSKzZ8/m3vvu4wtf+MICbcaNGzfv8aOPPcZq/fMV/FtusQWvv/46M2fOZO7cuYx67jnWXHNNlhUvvfoea/TvQr++nenYMdh1hz488OjUlldcRizr9bf3Y6e91//uY6NYcb0BdBmwOtGpE/2H7MGkm+5eqN2Kn1mHTt278s5DT82b13m1vtR1zl9uOnbvyipf3IIPXhmz0LpttdhjQIuxnGeQT2833Az9EeCkiNghpXRf0W4FYA/mX+DwEPAR+VTbMU1sfiz5FGGl3Ra31kbcRb7o6IkiPLTFstIjejb54paWTt0PJY85vAgYRz5d2py7gGMjYs+UUmP3L224KmLeV++I+CL5YponKtot7n4aC4wnh49bKuYPBt4DRlXMuxI4JvLN03ckX6wD5KEgEfEw8JnK8ZjNSSk9GxE/B74FbMj8096NGUrT+7W1+6iWGno+m7uPy1gqAmoxDrV6VPtd5Pfm+UW99VFK6a2IuJ18YdF2wO5VTbqw4D7sQMUdIJZFHTp0ZO8Df8mFpx9Gqq9nq532ZtXV1+PWq3/PGutswiZb7sJNl/+GWTM/5NLf5Rs7dO/Zj4OP+QN1dR0Y9K2f8+dTv0cisfraG7PNLtXXui3t+jvwwx/+kBNOOIG59fXsttturLXWWlxy6aVssP76fOELX2D48OE89fTTdOzYkZVWWomjj85DtVdeeWX+a++9OeLII4kItho4kK23XmrXey2yufVwzgWjOeekzairC26+cyJj3vyw1mW12rJef3s/dtp7/WnuXJ474mS2vvmvRIcOjB16LTNeGM0Gvzqc6U88x+QijPYf/FXGX/WvBdZdacN12eis4yAliOC1cy/m/edeWeI1Rkv5K/Ltco5k/h+Dlcnj5X5IvgBg95TSExXtHyCfyj6O3Ht2TNH+8yml0UWbX5DHbp0P/AtYnhxST0opjYuIPcjjRs8jj+XamRyuBgCbpZSei4gB5HGbgyqDUUS8DlyTUjqmYt5QYNOU0sBiegNyj+GD5J6rKeQxov9JvthlRHHRwj0Nz1exrRHAlJTSvsX0/5JP53+LIiyllMY3sS83BjYmB7FLyRdkjADerrhoay3g38DBzY0DrX5NjSxfiTz27qCU0tCqZTeR9/dpDRdwNfM8QQ5+XyQHrCfJPaI7pJS+H/km+KPJXz7OJPf0nUjuXX94MfdTAn6aUjq/mD6U3GN9NjnY7UgebnB8Sun0qnVfJV8E0xXok1L6sGLZduSAdBVwTbF/1iz2xS9TSq9ExEjyl6XnyL1oh5KvRt8wpTS2hX3V6H5dhH00lEbe08bmN3b8R8RdACmlJi91jHx3hb+Rx8d+RP4SuhE59I0Dtk0pzWli+2eRezGPIQ85OIQ87nuFlFKvok0v8jEyjnzngnHk3uodgZEppSta2If7ksf7jgXWqrx1WURcVdR9FLmn9cfkLwZrAyunlGY087ltav4IKj7PTRn+xJxa9UwvEZt0f6PWJSy27xzZ7MdOS9kl5y0TQ6Q/tV7c8Ku1LqFN9pj9cpN3amntKfhu5J7LB8l/HPYlXxm8WWX4LOxFDgnnFW0D2KUhfAKklE4jB9hdybf4+TPQnRwISCndTL5Sfl9yGFgLOKKVtbYopfQK+dZRH5LHk91C/gM8ixwUFsUfyeP7LgYeAw5rpu1g8j65tJj+cTF9UkWbII99Wzr3PchuKP5t8UKwood4b/J+OpK8r04hh3ZSSpPIvZOrkt/LI8m336nej4uyn6pruJD8/u9N/mLyDeDo6vBZGEYOyMMrw2exnZHknvXe5PdgOPkWXW8xf8zgQ+TTv9eQg2ov8t0SWvNX8Ibi3wX26yLso7bqUPy0xt3k1zoC+CV5X+yZUmrqlDbk4/Rq8vs/lHzBVvVrnUL+bL0EnEt+z88k/w5p+vLv+W4in1b/e1r4vrk/Jd9q7Q/k4+g5Fr76XZLUDrTYA6pPnqInqV9Kafta1/JJ4n5tu4j4KjmEblD5pbXW7AGtHXtAa8se0Nr6JPeAtuk+oGpfIv93iwPJ9z1cpsfOtSfu17aLiP7A+uR7tf5rWQqfkqQlzwD66TKcfEr5jymla1pqrFZzv7bdYcAJ5PGjP61xLZKkpcwA+imSUhpQ6xo+idyvbZdSOpF8YZYk6VNgaV7oIkmSJC3EACpJkqRSGUAlSZJUKgOoJEmSSmUAlSRJUqkMoJIkSSqVAVSSJEmlMoBKkiSpVAZQSZIklcoAKkmSpFIZQCVJklQqA6gkSZJKZQCVJElSqQygkiRJKpUBVJIkSaUygEqSJKlUBlBJkiSVygAqSZKkUhlAJUmSVCoDqCRJkkplAJUkSVKpDKCSJEkqlQFUkiRJpTKASpIkqVQGUEmSJJXKACpJkqRSGUAlSZJUKgOoJEmSSmUAlSRJUqkMoJIkSSqVAVSSJEmlMoBKkiSpVAZQSZIklcoAKkmSpFIZQCVJklQqA6gkSZJKZQCVJElSqQygkiRJKpUBVJIkSaUygEqSJKlUBlBJkiSVygAqSZKkUhlAJUmSVCoDqCRJkkplAJUkSVKpDKCSJEkqlQFUkiRJpTKASpIkqVQGUEmSJJXKACpJkqRSGUAlSZJUKgOoJEmSSmUAlSRJUqkMoJIkSSqVAVSSJEmlMoBKkiSpVAZQSZIklcoAKkmSpFJFSqnWNUjSMm3Gwze261+Uu5/ardYlLLbL+51b6xI+1b414ahal6B2bOTwHaOpZfaASpIkqVQGUEmSJJXKACpJkqRSGUAlSZJUKgOoJEmSSmUAlSRJUqkMoJIkSSqVAVSSJEmlMoBKkiSpVAZQSZIklcoAKkmSpFIZQCVJklQqA6gkSZJKZQCVJElSqQygkiRJKpUBVJIkSaUygEqSJKlUBlBJkiSVygAqSZKkUhlAJUmSVCoDqCRJkkplAJUkSVKpDKCSJEkqlQFUkiRJpTKASpIkqVQGUEmSJJXKACpJkqRSGUAlSZJUKgOoJEmSSmUAlSRJUqkMoJIkSSqVAVSSJEmlMoBKkiSpVAZQSZIklcoAKkmSpFIZQCVJklQqA6gkSZJKZQCVJElSqQygkiRJKpUBVJIkSaUygEqSJKlUBlBJkiSVygAqSZKkUhlAJUmSVCoDqCRJkkplAJUkSVKpDKCSJEkqlQFUkiRJpTKASpIkqVQGUEmSJJXKACpJkqRSGUAlSZJUKgOoJEmSSmUAlSRJUqkMoJIkSSqVAVSSJEmlMoBKkiSpVAZQSZIklcoAKkmSpFIZQCVJklQqA6gkSZJKZQBdiiLixIhIxU99RLwTEY9FxKkRsWqt66u1iBgcEQe2ol3XiDgpIh6NiHcjYmJEXB8RGyzm854YEVPa2qY9i4gREXFNC20GFMfunmXV1UQdUyLixFrWIElasgygS9+7wLbAF4H9geuAbwOjImLLWha2DBgMHNiKdmsChwK3AfsC3wf6AY9ExBpLrTpJkrRUdKx1AZ8Cc1JKD1dM3xYRfwLuA66MiA1TSnNrVFt7MQZYN6X0UcOMiLgfeBM4GDipVoVJLXnw2Zf4zeU3Mre+nr123JqD9tyl0XZ3PfYsx55/KZeeeDgbr70G49+exr6/OIu1+vUGYLN11+L4A/cps/QWbbPFKhxx6HrU1QU33TGBy655q9YlLaDzJp+nx5BDoK6OGSPv4L1br1uozQpbfolug/YHErPfep0pF51Dhx696f2j44iogw4deP/um5lx323Wv4Qt68dPS6y/bewBrYGU0nTgWGA94D8b5kdEr4j4e0RMjYgPi9OkA6vXj4hDI2JURMyMiEkRcU1EdCuWLXRqNSJ2Kk6lblpMN5xa3T8i/hYR70XE2Ig4oFh+bESMj4i3I+KMiKir2t6mEXFzRLxf/FxdOaSg4vl2KpbNiIjXIuJHFW2GAvsAO1YMUzixif31QWX4LOZNA94A+re8xxdfRHw+Ih4u3o+nImL7quXfiYiRETGtGGJxT+V7FhEHRsTHEdG9ar1Nite8a8W8r0fE48X7OjEizoyITi3Ut0dE3BERk4v38eGI2G0RXt9hEfF6RHxUvKertdA+RcRPquYtNFwhItaMiCuL/fJhRNwWEZ9pRT07RMQzxT54IiK+2ES7n0TEqxExKyJGR8RRjbTZr2jzUfG+fL6o/8CW6lhS5tbXc/ol1/O7o7/HNacdw20PP81r4yYt1O6Dj2Zyxe0j2XTdNReYv3qfnlzx659xxa9/tsyFz7o6+NkP1ueYE0dxwI8fY9cd+jBgjRVqXdZ8UUePb36fyb87mfG/+ikrbrU9nfqtvkCTjn360e0r+zDpzOOYcOLhTLvqIgDmvvsOE0//byb8+igmnnYs3Xbfhw7dVrH+JWiZP35aYP1LoIZSn02VRgBzgC9UzLsB+DJwDDCE/P7cExHrNTSIiBOAPwP3AnsBPySf5l9pMWo4A5hADoL3A3+PiLOBrck9i+eRg/LgiudfD3gA6AwcQD6FvgkwPCKiavsXAs8Aexev9w8RsXWx7NfAPcBT5CEK2wJ/bW3hEdGbHOBfqZrfZJBdDCsAfyfv732AWcB1EVH5KR0AXALsB3wTeAu4PyLWKZbfACTyPqg0BJhE3gdExGDy8IxHga+Re3UPA05roca1geHkYR37AA8Ct0TEl1rx+rYFfgr8DPge8Nmi3jaJiB7ASOAzwA/Ix8+KwJ0R0aWZ9foDtwDTyEMt/gxcTn4fKtsdCvweuBEYBFwNnB0Rx1W0GQhcCTxJ3vc3AsPa+toW1fOvvckafXuxep+edOrYkd222ZwRTz6/ULs/XXcb391jZ5bv1H5OSm20flfGTviI8ZNmMmdO4s77JrPdNj1rXdY8y629PnMmT2DOlEkwdw4fPDaSLp/bZoE2K22/G++P+Bf1H34AQP377+YFc+fAnDkARMdOUFf9q23pa+/1t2RZP35aYv1t135+233CpJRmFr1GfQEiYnfgS8BOKaV7i3l3A68DPwe+X/SiHQ+cl1L6WcXmFj4v0zp3p5SOL57rEfIf/a8BDcMCbo2Ir5P/gF9ZrPMrYCLwlZTSx8W6zwIvAV8Fbq7Y/hUppVOKNiPIYeG/gEdTSv+OiGlAXdUQhdY6G5gBDK2aPxeoX4ztNaYLcGRK6W6AiJhADsw7ALcCpJRObmhc9BTfQQ7wBwAnp5SmR8St5MD5t4ptDwGuSSnNLYL7WcAlKaXKXuJZ5NB+WkppamMFppTOr3r+e8hfCL5H/qLQnD7AtimlN4v13wBGRsTuKaVbW1i3OUeRA+fmRU81EfEA+Vg+GPhDE+sdCcwE9kgpfVis9wFwWUOD4jWeCAxNKR1dzL498hmAX0TEeSmlmcB/Ay8C+6eUEvlY7kT+0lWaye+8R98e3edN9+3Rjef+/eYCbV58fSyTpk1n+8034tJbRiywbNzb0/jm/5zLil0686N9vsznP7MOy4rePZdj8pRZ86bfnjqLjTfoWsOKFtSxew/mTJvfMT93+lSWW3v9Bdp06ptPoPQ99jSiro7pw69k5vNPAdBhlV70+ekJdOzTj3euGcrcd98pr3jaf/0tWdaPn5ZYf9vZA1pblV9LtwYmN4RPyKeegZuA7YpZ25JDUWWQaYu7Kp7rPeBt4N6qMamjgcrTsrsC1wP1EdExIjqSx2i+DlQPF7i9YvuzgVeB1WmjiPghOeAdUh3MUkodK0NhG31M7rlt8ELx77zXEBEbRb4ifxI5/M4m9/xVXqE/DPiPiOhZrLN5sbyhR24D8oVWVzXs02K/3k3uad60qQIjYvXIwzbGkXvUZwO7VT1/U55sCJ8AKaUHgMnkY7EtdiUH8fcqXsv7wBMsfIxU2hq4oyF8Fq6varM6edjF1VXzhwFdgc2K6a2A4UX4bHDjIr2KEtTX13PuFcM5av9BCy3r1b0rN5/7S/7x66P42TcG8csL/sGMj2bWoMpPsLo6OvXpx6SzT2DKhWfT89s/JrqsCMDcd6Yw4eQjGf/LH7DStjtTt3K3GhfbiPZevz7VDKA1EhGdgZ7k07CQr+qe3EjTSUCP4nFD//iEJVTG9Krpj5uY17liuhe5d2l21c86QPUV6S1ta5FFxNfIp1//O6VUHU6WtPdTSvN6Uxt6fCleQ0SsTA7Za5BPY29PDj7PsODrvJG8jxoG8Q0BxpJPU0PepwD/YsF9OqaY3+iV/kVv4I3kOyz8L7Bz8fy30Lr93NjxNpl8LLZFL/JrrD5GdqaJ11JYtbqmIozOqJjVUFv1QMqG6YbPyqrkL1SVqqebVYyPfTwiHr/4hsW7gKPPKl2ZNG36/CKnvUvvVeYHgQ9mzmL02IkcdvoF7Hn0/zHq329y1HlDeWHMWyzXqSPdV8phYqO1V2f1Pj15c+IivYSl6u2pH9On1/Lzpnv3XJ63p85qZo1yzZk+jY49es2b7tC9J3PfmbZAm7nvTOXDZx6FuXOZM3UysyeNp1OfBQ//ue++w+zxb9J5/Y1LqbtBe6+/Jcv68dMS6287A2jt7EweAvFQMT2BfEq0Wl/ymDiAht6+5gLCTGC5qnlLcvT5NPLYvK0a+TllCT7PQopxjVcCF6SUzlqaz9VK25J75A5IKV2eUhqZUnocWKCrIaU0gzw0YUgxazBwdUXvXMP7exiN79dbmnj+9YDPAz9NKV2UUrq3eP4mx1lWaex460PzX3Bm0fLxNY0cjBt7LT9uZtsTq2sqxttWjm9uqK269r4Vz92wrd5Vbaqnm5VS+ktKaWBKaeDBe315UVadZ+O11+CtSVMY9/Y0Zs+Zw+2PPM2On58fBFZeoQt3/+Ekbjr7eG46+3g2W3dNzj3yQDZeew3eeW8Gc+vz95+xk6fy5sQprNZ72Rlj9tKr77FG/y7069uZjh2DXXfowwOPNjpSpCY+fv1VOvbpR8eefaBDR1bcajs+eubRBdp8+PQjdN4gn2CoW2llOvXtz5wpk+jQvSfRKR/mdSusyPLrbcTsSeOtfwla1o+fllh/2zkGtAaKsZxnkE9v31nMfgQ4KSJ2SCndV7RbAdiD+achHwI+Ar5LvlCpMWPJYxQrtfqq6Fa4izzG8Imq05uLo9U9ohGxCflim1uBw9v4vEtKQ9Cb97WxuGp7APl0c6UrgWERMYjcW3xlxbKXgXHAgJTShW18/rXIY4mfbcX6W0TEmhVjQL9EDnaPNrPOWGCjiuerA/6jqs1d5JD9fPXdC1rwGHBwRKxQcRq++uKtscB48kVflcF8MPAeMKpiW4Mi4viK4/Rri1DLEtGxQweO/fZe/OSsC5lbX8/Xd9iadVdflT9ddxsbD1idHbfYpMl1n3z5NS647nY6dqwjoo7jD9yHbistO1fZzq2Hcy4YzTknbUZdXXDznRMZ8+aHLa9Ylvp6pl1xIX2O/BXUdWDGA3cye8JbdPvaN/j4jdF89MxjzHz+KbpsvDn9Tvw9pHreuXYo9R+8T+eNPscq+x0EKUEE793+T2aPe8P6l6Bl/vhpgfW3XbQ9Q6gpxdXYRwK7F7NWBrYkX7m+ArB7SumJivYPkMPJceTezmOK9p9PKY0u2vwCOBU4n3zKdnlySD0ppTQuIvYgjxs9j9zrtjP56uwBwGYppeciYgD59O6glNJNFc//OvnCmGMq5g0FNk0pDSymNyAHlAeBi4Ep5DGi/0m+MGREROxEvhhms5TScxXbGgFMSSntW0z/L/l0/rcogkVKaaGv6RHRhxzoEvAdci9vg/dSSi9UtJ1DvvinyXGgxfvyk5RSr0VtExGJ3ON4fkT0JX+JeAQ4k9wbeiL5zMLDDa+zWK8L+TTxDGBmSmmdqu0OAS4l9y7fQg7n65DvdLBv1bjIhnWWL57/beB/yMfXSeQeyqkN71kTr28EeZzoVPKFZZ3JX4omp5S2LNoMoOo4iYizyL2YxwCvAYeQx26u0LCvIqIX+erzceThEuPIPZQ7AiNTSlc0UdNq5HHCDwPnkMd6/qJY98yU0olFu0OL/XQ2eazpjkW741NKpxdtBpLfl2vIY6Y3An5E7jX+bkrpkqLdRcCOKaV5d5pozIyHb2zXvyh3P7X9jv+7vN+5tS7hU+1bExa6w5nUaiOH79jkLRg8Bb/0dSP3XD5IvnBiX/JVvZtVhs/CXuQ/qOcVbQPYpSF8AqSUTiMH2F2Bf5L/EHcnX+RBSulm8pXy+5J7TtcCjlhSLyal9Ar51lEfAn8hh6WTyL1wo5tZtTF/JI+hvJjcY3VYE+02Joe7NcjB9qGKnz9Wte1AScd1SmkSuSduVfJ7cST5tkML7YeiJ/BG8vCJhW4HlFIaBnwd2Jz83l9HDkxPksNoY88/i3xXgTnkoPVr8m2b7m2sfSMeJF+Rfh5wEfAc+RhszklFfaeQ70DwNFUXxaWUppCPkZeAc8nv8Znkz0KTPbMppXHkOyn0Aq4lv/4DyMdaZbsLycf03uQvW98Ajm4In0Wbx4v5W5JvLbUP+XMDuae0QQc8EyRJpbMHVNKnQuT/aOFSYJ2U0piW2leyB7R27AGtLXtA1RbN9YD6zV/SJ1Lk//L2DuAdYAvgBODmRQ2fkqQlzwAq6ZOqJ3mIRk/yWNdh5P/ZS5JUYwZQSZ9IKaXBLbeSJNWCFyFJkiSpVAZQSZIklcoAKkmSpFIZQCVJklQqA6gkSZJKZQCVJElSqQygkiRJKpUBVJIkSaUygEqSJKlUBlBJkiSVygAqSZKkUhlAJUmSVCoDqCRJkkplAJUkSVKpDKCSJEkqlQFUkiRJpTKASpIkqVQGUEmSJJXKACpJkqRSGUAlSZJUKgOoJEmSSmUAlSRJUqkMoJIkSSqVAVSSJEmlMoBKkiSpVAZQSZIklcoAKkmSpFIZQCVJklQqA6gkSZJKZQCVJElSqQygkiRJKpUBVJIkSaUygEqSJKlUBlBJkiSVygAqSZKkUhlAJUmSVCoDqCRJkkplAJUkSVKpDKCSJEkqlQFUkiRJpTKASpIkqVQGUEmSJJXKACpJkqRSGUAlSZJUKgOoJEmSSmUAlSRJUqkMoJIkSSqVAVSSJEmlMoBKkiSpVAZQSZIklcoAKkmSpFIZQCVJklQqA6gkSZJKZQCVJElSqQygkiRJKpUBVJIkSaUygEqSJKlUBlBJkiSVygAqSZKkUkVKqdY1SNKnWkQcllL6S63rWFzWX1vtuf72XDtYf1vYAypJtXdYrQtoI+uvrfZcf3uuHax/sRlAJUmSVCoDqCRJkkplAJWk2mu3Y8gK1l9b7bn+9lw7WP9i8yIkSZIklcoeUEmSJJXKACpJS0lEDIiI5xqZPyIiBtaipjJExE4R8cVa19GYiDgxIo6pdR3Naeq4aQ8iYkata1D7YACVJC1pOwHLZACV1LjISsuFBlBJWro6RsTlEfFiRFwTEStULqzsMYqIfSNiaPG4d0RcGxGPFT9fKrnuhUTEdyLi2Yh4JiIujYhBEfFIRDwVEXdGRN+IGAD8ADgqIp6OiO1rXDYR8cuIeCUiRgKfKeZtHhEPF6/n+ohYpZi/VTHv6Yg4q4Y9kR0i4sKIeD4ibo+ILhFxaHEsPFMcGytERLeIeKMhOETEihHxVkR0ioh1I+LWiHgiIu6PiA3LKr4IM2dFxHMRMSoihhTzr4yIPSraDS2O+w5F+8eK/f/9Ynm/iLiveD+eq8XxVPRIv1TU+krxed41Ih6IiFcjYuvi395F+7qIGN0wXYN6T46IIyumT42IIyLiroh4sng/vl7x2l6OiEuA54Dti99VCxx7S6XQlJI//vjjjz9L4QcYACTgS8X0xcAxwAhgYDFvRkX7fYGhxeN/ANsVj9cEXqzxa9kEeAXoVUz3AFZh/sWshwBnF49PBI6p9f4vatkSGAWsAHQFRhfvwbPAjkWbk4HzisfPAdsWj08HnqvRcTMH2LyYvgo4AOhZ0eYU4KfF438COxePhwB/LR7fBaxfPN4GuLuE2mcU/+4D3AF0APoCbwL9gL2BvxdtlgPeArqQb4h+QjF/eeBxYG3gaOCXxfwOwMo1fD82I3fcPVF8lgP4OnAD8CvgyKL9bsC1NTzmBwBPFo/rgH8X70HXYl6v4nMQRdt64AvNHXtLo86OSJKWprdSSg8Ujy8DDm/lersCG0dEw3TXiFgppVSrMXa7AFenlKYApJSmRcRmwLCI6EcOE2NqVFtztgeuTyl9CBARNwIrAt1TSvcWbf4OXB0R3ckB56Fi/j+APUuut8GYlNLTxeMnyMFg04g4BegOrATcViwfRg6e9wD7A3+MiJXIwyCurjiGli+j8MJ2wBUppbnApIi4F9gKuAX4bUQsD+wO3JdS+igidgM+GxH7Fut3A9YHHgMujohOwA0V+6RsY1JKowAi4nngrpRSiohR5Pfmp+QvAucBBwN/q1GdpJRej4ipEfF5cvB8CpgGnBsRO5AD52rFMoA3UkoPV2yisWNviTOAStLSVX2vu+amO1c8riP3SsxcKlUtGb8Hzkkp3RgRO5F7PrVkzKp4PJfcSzgU2Cul9ExEHEgeawtwI/B/EdGD3ON7NzlkT08pbV5Sva2SUpoZESOAL5ND85XFoiD36N5WvU4RmvYAhkbEOSmlS8qqt0Ll+1FfMV0PdEwpvRURkyJiF2Br4FtlF1jlr8CBwKrk3tpvAb2BLVNKsyPideb/vvmgat3Gjr0lzjGgkrR0rRkR2xaPvwmMrFo+KSI2Ksbw7V0x/3ZyrwqQxywu1SpbdjewX0T0BCjCTjdgXLH8uxVt3wdWLre8Jt0H7FWMoVwZGET+g/tOxXjCbwP3ppSmA+9HxDbF/P1Lr7Z5KwMTit7AeQGn6BV/DPgtcFNKaW5K6T1gTETsB/PGZH6uxFrvB4YUYzt7AzsAjxbLhgEHkXunby3m3Qb8sHhtRMQGxXjWtYBJKaULyaFqixJfw6L6K/ksx9VFz28tXU/uYd6KvG+7AZOL8LkzsFYtiwMDqCQtbS8DP46IF8ljJv9Utfw44CbgQWBCxfzDgYHFBRkvkC/sqZmU0vPAqcC9EfEMcA65x/PqiHgCmFLRfDiwdywDFyGllJ4kB55nyKd/HysWfRc4KyKeBTYnjwMF+B5wYUQ8Te5FfLfMelvwP8AjwAPAS1XLhpHHiQ6rmPct4HvF+/U8ebxiWa4nj7N9hvzl5diU0sRi2e3AjsCdKaWPi3l/BV4Anox84defyWdpdwKeiYinyD2mvy3tFSy6G8lDI2p2+r1BsV/vAa4qwvDl5N8no4DvsPDxUzr/JyRJkgqV42wj4jigX0rpiBqXpXYg8r19z00pLQt3fqgDngT2Sym9Wut6GmMPqCRJ8+3RcMsf8iniU2pdkJZ9xZeVa4FfLAO1bEy+yv2uZTV8gj2gkiRJKpk9oJIkSSqVAVSSJEmlMoBKkiSpVAZQSZIklcoAKkmSpFIZQCVJklSq/wdXbLOHknDNZQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "document_collection = [\n",
    "    'My cat loves yarn. Blue yarn.',\n",
    "    'I have a blue dog.'\n",
    "]\n",
    "\n",
    "# 1. Create a TfidfVectorizer oject\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# 2. Fit the vectorizer to document_collection\n",
    "vectorizer.fit(document_collection)\n",
    "\n",
    "# 3. Print the vocabulary\n",
    "print(\"Vocabulary size {0}: {1}\\n\".format(len(vectorizer.vocabulary_), vectorizer.vocabulary_))\n",
    "\n",
    "# 4. Transform the data into numerical vectors \n",
    "print(\"Matrix:\\n\")\n",
    "resulting_matrix = vectorizer.transform(document_collection)\n",
    "\n",
    "# 5. Print the matrix\n",
    "print(resulting_matrix.todense())\n",
    "\n",
    "# 6. Visualize the matrix in a heatmap\n",
    "print(\"\\nHeatmap of Matrix:\\n\")\n",
    "df_print = pd.DataFrame(resulting_matrix.toarray(), columns=vectorizer.get_feature_names())\n",
    "plt.rcParams['figure.figsize'] = [6, 5]  \n",
    "ax = sns.heatmap(df_print, annot=True, cmap='coolwarm', cbar=False, yticklabels=[\"Document 1: My cat loves yarn. Blue yarn\", \"Document 2: I have a blue dog.\"]);\n",
    "_ =ax.set_title('TF-IDF Matrix');\n",
    "_ =ax.set_yticklabels(ax.get_yticklabels(), rotation = 0, fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's summarize the resulting matrix above:\n",
    "\n",
    "<table>\n",
    "    <tr><th></th><th>blue</th><th>cat</th><th>dog</th><th>have</th><th>loves</th><th>my</th><th>yarn</th></tr>\n",
    "<tr><th>Document 1</th><th>0.25969799</th><th>0.36499647</th><th>0.</th><th>0.</th><th>0.36499647</th><th>0.36499647</th><th>0.72999294</th><t/tr>\n",
    "<tr><th>Document 2</th><th>0.44943642</th><th>0.</th><th>0.6316672</th><th>0.6316672</th><th>0.</th><th>0.</th><th>0.</th></tr>   \n",
    "    </table>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We have 7 words in our vocabulary: 'blue', 'cat', dog', 'have', 'loves', 'my, 'yarn'. Note that scikit-learn excluded the words 'I' and 'a'. Therefore, we have 7 columns. Note that each word is considered a feature. Therefore, in this example, we have seven features.\n",
    "\n",
    "The `vectorizer.vocabulary_` attribute outputs a mapping of words to column indices: {'my': 5, 'cat': 1, 'loves': 4, 'yarn': 6, 'blue': 0, 'have': 3, 'dog': 2}. This means that the TF-IDF score (weight) for the word 'my' is contained is in the 5th column (the first column is 0) in the matrix.\n",
    "\n",
    "The table above summarizes the results of the code. Note that in our first document, the word 'dog' does not appear. Therefore, its value in the document's vector is 0. Since the word 'blue' appears in both documents, its importance is not as high for either document. Therefore, its value in both document vectors is not very high compared to other values. However, since 'dog' appears in the second document only, it has a higher importance since it is characteristic of the second document; its value in that document's vector is 0.6316672. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now transform our book review textual features into numerical vectors using `TfidfVectorizer`. We will implement a TF-IDF transformation on the training and test data. Run the cell and inspect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size 18558: \n",
      "[('there', 16673), ('is', 9043), ('reason', 13533), ('this', 16714), ('book', 2189), ('has', 7803), ('sold', 15423), ('over', 11793), ('180', 73), ('000', 1), ('copies', 3867), ('it', 9076), ('gets', 7240), ('right', 14207), ('to', 16835), ('the', 16627), ('point', 12568), ('accompanies', 444), ('each', 5372), ('strategy', 15943), ('with', 18277), ('visual', 17844), ('aid', 750), ('so', 15386), ('you', 18497), ('can', 2604), ('get', 7239), ('mental', 10534), ('picture', 12402), ('in', 8491), ('your', 18501), ('head', 7844), ('further', 7051), ('its', 9088), ('section', 14743), ('on', 11601), ('analyzing', 974), ('stocks', 15886), ('and', 984), ('commentary', 3384), ('state', 15782), ('of', 11543), ('financial', 6568), ('statements', 15786), ('market', 10286), ('are', 1220), ('money', 10863), ('if', 8336), ('just', 9282), ('starting', 15774)]\n",
      "\n",
      "[[0.         0.16185315 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.01923341 0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# 1. Create a TfidfVectorizer oject\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# 2. Fit the vectorizer to X_train\n",
    "tfidf_vectorizer.fit(X_train)\n",
    "\n",
    "# 3. Print the first 50 items in the vocabulary\n",
    "print(\"Vocabulary size {0}: \".format(len(tfidf_vectorizer.vocabulary_)))\n",
    "print(str(list(tfidf_vectorizer.vocabulary_.items())[0:50])+'\\n')\n",
    "\n",
    "      \n",
    "# 4. Transform *both* the training and test data using the fitted vectorizer and its 'transform' attribute\n",
    "X_train_tfidf = tfidf_vectorizer.transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "# 5. Print the matrix\n",
    "print(X_train_tfidf.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Fit a Logistic Regression Model to the Transformed Training Data and Evaluate the Model\n",
    "The code cell below trains a logistic regression model using the TF-IDF features and computes the AUC on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on the test data: 0.9146\n",
      "The size of the feature space: 18558\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('is', 9043), ('reason', 13533), ('this', 16714), ('book', 2189)]:\n"
     ]
    }
   ],
   "source": [
    "# 1. Create a LogisticRegression model object, and fit a Logistic Regression model to the transformed training data\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# 2. Make predictions on the transformed test data using the predict_proba() method and \n",
    "# save the values of the second column\n",
    "probability_predictions = model.predict_proba(X_test_tfidf)[:,1]\n",
    "\n",
    "# 3. Make predictions on the transformed test data using the predict() method \n",
    "class_label_predictions = model.predict(X_test_tfidf)\n",
    "\n",
    "# 4. Compute the Area Under the ROC curve (AUC) for the test data. Note that this time we are using one \n",
    "# function 'roc_auc_score()' to compute the auc rather than using both 'roc_curve()' and 'auc()' as we have \n",
    "# done in the past\n",
    "auc = roc_auc_score(y_test, probability_predictions)\n",
    "print('AUC on the test data: {:.4f}'.format(auc))\n",
    "\n",
    "# 5. Print out the size of the resulting feature space using the 'vocabulary_' attribute of the vectorizer\n",
    "len_feature_space = len(tfidf_vectorizer.vocabulary_)\n",
    "print('The size of the feature space: {0}'.format(len_feature_space))\n",
    "\n",
    "# 6. Get a glimpse of the features:\n",
    "first_five = list(tfidf_vectorizer.vocabulary_.items())[1:5]\n",
    "print('Glimpse of first 5 entries of the mapping of a word to its column/feature index \\n{}:'.format(first_five))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check two book reviews and see if our model properly predicted whether the reviews are good or bad reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review #1:\n",
      "\n",
      "I've been a fan of Carol Dweck's scholarly work for years. Her work on self-esteem, self-concept, and the incremental vs. entity theories of intelligence provides some of the most powerfully useful tools I've encountered for educators and parents in their work with children, as well as in their own self-awareness and lives. I'm delighted to see this information written here in such a user-friendly conversational tone, rich with stories that illustrate the nuances and complexities of Dweck's research and ideas. I'm recommending this book to all of my graduate students (teachers and principals working with gifted learners), as well as to parents of high-ability children.\n",
      "\n",
      "Dona Matthews, Ph.D., Director of the Hunter College Center for Gifted Studies and Education, City University of New York\n",
      "\n",
      "\n",
      "Prediction: Is this a good review? True\n",
      "\n",
      "Actual: Is this a good review? True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Review #1:\\n')\n",
    "print(X_test.to_numpy()[124])\n",
    "\n",
    "print('\\nPrediction: Is this a good review? {}\\n'.format(class_label_predictions[124])) \n",
    "\n",
    "print('Actual: Is this a good review? {}\\n'.format(y_test.to_numpy()[124]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review #2:\n",
      "\n",
      "I have read other books by Alesia Holliday and enjoyed them so I looked forward to reading this book.  Unfortunately, I could not get any farther than the first 25 pages.  I even tried diving in further into the book to see if it got better and I still could not read more than 5 pages without turning away.  The best I can do to pin down why I dislike it so much is to say that it tries too hard.  No character seems to even approach reality.  They are all, including the main character and her love interest, over the top\n",
      "\n",
      "\n",
      "Prediction: Is this a good review? False\n",
      "\n",
      "Actual: Is this a good review? False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Review #2:\\n')\n",
    "print(X_test.to_numpy()[238])\n",
    "\n",
    "print('\\nPrediction: Is this a good review? {}\\n'.format(class_label_predictions[238])) \n",
    "\n",
    "print('Actual: Is this a good review? {}\\n'.format(y_test.to_numpy()[238]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Experiment with Different Document Frequency Values and Analyze the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating a `TfidfVectorizer` object, you can use the parameter `min_df` to specify the minimum 'document frequency.' This allows you to ignore words that have a document frequency lower than the specified value. In other words, they ignore words that occur in too few documents.\n",
    "\n",
    "The code cell below puts the code above into a loop over a range of 'document frequency' values. For each value, it fits a vectorizer specifying `ngram_range=(1,2)` (instead of the default (1,1)). Run the code and inspect the results. \n",
    "\n",
    "Note: This may take a short while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Min Document Frequency Value: 1\n",
      "AUC on the test data: 0.9268\n",
      "The size of the feature space: 138486\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('is', 61671), ('reason', 97323), ('this', 120815), ('book', 18054)]:\n",
      "Glimpse of first 5 stop words \n",
      "[]:\n",
      "\n",
      "Min Document Frequency Value: 10\n",
      "AUC on the test data: 0.9195\n",
      "The size of the feature space: 4023\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('is', 1687), ('reason', 2699), ('this', 3396), ('book', 464)]:\n",
      "Glimpse of first 5 stop words \n",
      "['counted as', 'delivery and', 'low 80s', 'case histories']:\n",
      "\n",
      "Min Document Frequency Value: 100\n",
      "AUC on the test data: 0.8463\n",
      "The size of the feature space: 257\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('is', 102), ('this', 207), ('book', 35), ('has', 81)]:\n",
      "Glimpse of first 5 stop words \n",
      "['counted as', 'delivery and', 'low 80s', 'case histories']:\n",
      "\n",
      "Min Document Frequency Value: 1000\n",
      "AUC on the test data: 0.6435\n",
      "The size of the feature space: 9\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('this', 7), ('book', 1), ('it', 4), ('to', 8)]:\n",
      "Glimpse of first 5 stop words \n",
      "['counted as', 'delivery and', 'low 80s', 'case histories']:\n"
     ]
    }
   ],
   "source": [
    "for min_df in [1,10,100,1000]:\n",
    "    \n",
    "    print('\\nMin Document Frequency Value: {0}'.format(min_df))\n",
    "    \n",
    "    # 1. Create a TfidfVectorizer oject\n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df=min_df, ngram_range=(1,2))\n",
    "\n",
    "    # 2. Fit the vectorizer to X_train\n",
    "    tfidf_vectorizer.fit(X_train)\n",
    "\n",
    "    # 3. Transform the training and test data\n",
    "    X_train_tfidf = tfidf_vectorizer.transform(X_train)\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "    # 4. Create a LogisticRegression model object, and fit a Logistic Regression model to the transformed \n",
    "    # training data\n",
    "    model = LogisticRegression(max_iter=200)\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    # 5. Make predictions on the transformed test data using the predict_proba() method and save \n",
    "    # the values of the second column\n",
    "    probability_predictions = model.predict_proba(X_test_tfidf)[:,1]\n",
    "\n",
    "    # 6. Compute the Area Under the ROC curve (AUC) for the test data.\n",
    "    auc = roc_auc_score(y_test, probability_predictions)\n",
    "    print('AUC on the test data: {:.4f}'.format(auc))\n",
    "\n",
    "    # 7. Compute the size of the resulting feature space using the 'vocabulary_' attribute of the vectorizer\n",
    "    len_feature_space = len(tfidf_vectorizer.vocabulary_)\n",
    "    print('The size of the feature space: {0}'.format(len_feature_space))\n",
    "    \n",
    "    # 8. Get a glimpse of the features:\n",
    "    first_five = list(tfidf_vectorizer.vocabulary_.items())[1:5]\n",
    "    print('Glimpse of first 5 entries of the mapping of a word to its column/feature index \\n{}:'.format(first_five))\n",
    "\n",
    "    # 9: Print the first five \"stop words\" - words that we are ignoring\n",
    "    first_five_stop = list(tfidf_vectorizer.stop_words_)[1:5]\n",
    "    print('Glimpse of first 5 stop words \\n{}:'.format(first_five_stop))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Analysis:</b> Just as you can use the parameter `min_df` to specify the minimum 'document frequency,' you can use the parameter `max_df` to ignore words that have a document frequency higher than the specified value. Try using the parameter `max_def` and compare the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
